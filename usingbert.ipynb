{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_tweet</th>\n",
       "      <th>main_tweet_id</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers</th>\n",
       "      <th>replies</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 regularly rinsing nose saline help prevent i...</td>\n",
       "      <td>1250219300389974016</td>\n",
       "      <td>False</td>\n",
       "      <td>410</td>\n",
       "      <td>[{\"tweet_id\": 1250219116993974272, \"tweet\": \"4...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french police chief killed charliehebdo attack</td>\n",
       "      <td>554886875303780352</td>\n",
       "      <td>True</td>\n",
       "      <td>3229894</td>\n",
       "      <td>[{\"tweet_id\": 554959644125167617, \"tweet\": \"De...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus disease covid19 advice public‚ú≥Ô∏è we...</td>\n",
       "      <td>1237901309011021825</td>\n",
       "      <td>False</td>\n",
       "      <td>613</td>\n",
       "      <td>[{\"tweet_id\": 1237901311439450112, \"tweet\": \"I...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ottawa police confirm multiple suspects shooti...</td>\n",
       "      <td>524958128392376320</td>\n",
       "      <td>True</td>\n",
       "      <td>19783124</td>\n",
       "      <td>[{\"tweet_id\": 524961934064754688, \"tweet\": \"@W...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>primary focus government isnt alleviate suffer...</td>\n",
       "      <td>1239295488677085185</td>\n",
       "      <td>False</td>\n",
       "      <td>4889</td>\n",
       "      <td>[]</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>4 cannot transmitted goods manufactured china ...</td>\n",
       "      <td>1237545128828342277</td>\n",
       "      <td>False</td>\n",
       "      <td>631</td>\n",
       "      <td>[{\"tweet_id\": 1237545126278258703, \"tweet\": \"#...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>desperate ted cruz claims planned parenthood s...</td>\n",
       "      <td>671181758692507648</td>\n",
       "      <td>True</td>\n",
       "      <td>143090</td>\n",
       "      <td>[{\"tweet_id\": 671200376843067392, \"tweet\": \"@B...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>thoughts prayers enough pres obama speaks mass...</td>\n",
       "      <td>672513234419638273</td>\n",
       "      <td>True</td>\n",
       "      <td>17449031</td>\n",
       "      <td>[{\"tweet_id\": 672513853645717504, \"tweet\": \"@A...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>police surrounded building suspected charliehe...</td>\n",
       "      <td>553508098825261056</td>\n",
       "      <td>True</td>\n",
       "      <td>9077962</td>\n",
       "      <td>[{\"tweet_id\": 553509546602553344, \"tweet\": \"@N...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>lynnesimpkin help üë©‚Äçüè´9am socialism  need bad10...</td>\n",
       "      <td>1241082793737818113</td>\n",
       "      <td>False</td>\n",
       "      <td>3103</td>\n",
       "      <td>[{\"tweet_id\": 1241041450084839424, \"tweet\": \"E...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             main_tweet        main_tweet_id  \\\n",
       "0     5 regularly rinsing nose saline help prevent i...  1250219300389974016   \n",
       "1      french police chief killed charliehebdo attack     554886875303780352   \n",
       "2     coronavirus disease covid19 advice public‚ú≥Ô∏è we...  1237901309011021825   \n",
       "3     ottawa police confirm multiple suspects shooti...   524958128392376320   \n",
       "4     primary focus government isnt alleviate suffer...  1239295488677085185   \n",
       "...                                                 ...                  ...   \n",
       "1889  4 cannot transmitted goods manufactured china ...  1237545128828342277   \n",
       "1890  desperate ted cruz claims planned parenthood s...   671181758692507648   \n",
       "1891  thoughts prayers enough pres obama speaks mass...   672513234419638273   \n",
       "1892  police surrounded building suspected charliehe...   553508098825261056   \n",
       "1894  lynnesimpkin help üë©‚Äçüè´9am socialism  need bad10...  1241082793737818113   \n",
       "\n",
       "      verified  followers                                            replies  \\\n",
       "0        False        410  [{\"tweet_id\": 1250219116993974272, \"tweet\": \"4...   \n",
       "1         True    3229894  [{\"tweet_id\": 554959644125167617, \"tweet\": \"De...   \n",
       "2        False        613  [{\"tweet_id\": 1237901311439450112, \"tweet\": \"I...   \n",
       "3         True   19783124  [{\"tweet_id\": 524961934064754688, \"tweet\": \"@W...   \n",
       "4        False       4889                                                 []   \n",
       "...        ...        ...                                                ...   \n",
       "1889     False        631  [{\"tweet_id\": 1237545126278258703, \"tweet\": \"#...   \n",
       "1890      True     143090  [{\"tweet_id\": 671200376843067392, \"tweet\": \"@B...   \n",
       "1891      True   17449031  [{\"tweet_id\": 672513853645717504, \"tweet\": \"@A...   \n",
       "1892      True    9077962  [{\"tweet_id\": 553509546602553344, \"tweet\": \"@N...   \n",
       "1894     False       3103  [{\"tweet_id\": 1241041450084839424, \"tweet\": \"E...   \n",
       "\n",
       "          label  \n",
       "0     nonrumour  \n",
       "1        rumour  \n",
       "2     nonrumour  \n",
       "3     nonrumour  \n",
       "4     nonrumour  \n",
       "...         ...  \n",
       "1889  nonrumour  \n",
       "1890     rumour  \n",
       "1891     rumour  \n",
       "1892  nonrumour  \n",
       "1894  nonrumour  \n",
       "\n",
       "[1567 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "train_file = \"train.data.txt\"\n",
    "dev_file = \"dev.data.txt\"\n",
    "test_file = \"test.data.txt\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def tokenize_tweet(string_data:str):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = nltk.RegexpTokenizer('\\w+')\n",
    "    data = string_data.replace('\\n', '')\n",
    "    data = data.lower()\n",
    "    data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n",
    "    data = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "    # data = [wordnet_lemmatizer.lemmatize(word) for word in data.split(' ')]\n",
    "    data = ' '.join([i for i in data.split(' ') if i not in stopwords])\n",
    "    return data\n",
    "\n",
    "train_data = pd.read_csv('./%s.csv'%train_file,keep_default_na=False)\n",
    "dev_data = pd.read_csv('./%s.csv'%dev_file,keep_default_na=False)\n",
    "test_data = pd.read_csv('./%s.csv'%test_file,keep_default_na=False)\n",
    "def preprocess_token(df, dropNa=True):\n",
    "    data = df.copy()\n",
    "    data['main_tweet'] = data['main_tweet'].fillna('')\n",
    "    if dropNa:\n",
    "        data.replace('', np.nan, inplace=True)\n",
    "        data.dropna(subset=['main_tweet'], inplace=True)\n",
    "    text = data['main_tweet'].apply(lambda x: tokenize_tweet(x))\n",
    "    data['main_tweet'] = text\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data=preprocess_token(train_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (4.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: filelock in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: sacremoses in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (2022.3.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: six in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kan/miniconda3/envs/pydml/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 511M/511M [01:38<00:00, 5.42MB/s]\n",
      "2022-05-05 01:36:27.718245: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2022-05-05 01:36:27.923604: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2022-05-05 01:36:27.962410: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10274/1582137241.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pydml/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         raise ValueError(\n\u001b[1;32m    448\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pydml/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0;31m# see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1339-L1357\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m             missing_keys, unexpected_keys, mismatched_keys = load_tf_weights(\n\u001b[0m\u001b[1;32m   1810\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 \u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pydml/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mload_tf_weights\u001b[0;34m(model, resolved_archive_file, ignore_mismatched_sizes, _prefix)\u001b[0m\n\u001b[1;32m    596\u001b[0m                         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0msaved_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5_layer_object\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                     \u001b[0;31m# Add the updated name to the final list for computing missing/unexpected values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pydml/lib/python3.8/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pydml/lib/python3.8/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mread_direct\u001b[0;34m(self, dest, source_sel, dest_sel)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_sel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_label = le.fit_transform(train_data['label'])\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,do_lower_case=True)\n",
    "train_text = train_data['main_tweet'].tolist()\n",
    "encoding = tokenizer(train_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "dev_data=preprocess_token(dev_data)\n",
    "dev_text = dev_data['main_tweet'].tolist()\n",
    "dev_encode = tokenizer(dev_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "dev_label = le.transform(dev_data['label'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = TFAutoModel.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "print(encoding[\"input_ids\"].shape)\n",
    "\n",
    "def create_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(100,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(100,), name='attention_mask', dtype='int32')\n",
    "\n",
    "    embeddings = model(input_ids, attention_mask=mask)[0]\n",
    "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "    X = tf.keras.layers.Dropout(0.1)(X)\n",
    "    y = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(X)\n",
    "\n",
    "    tfmodel = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "    return tfmodel\n",
    "\n",
    "tfmodel = create_model()\n",
    "# optimizer = tf.keras.optimizers.Adam(2e-5)\n",
    "# tfmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# history = tfmodel.fit({\"input_ids\": encoding[\"input_ids\"], 'attention_mask': encoding[\"attention_mask\"]}, \\\n",
    "#                       train_label,\\\n",
    "#                       validation_data=({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]}, dev_label),\\\n",
    "#                       epochs=10)\n",
    "tfmodel.load_weights('bert-weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfmodel.save(\"model.h5\")\n",
    "# dev_data=preprocess_token(dev_data)\n",
    "\n",
    "# dev_text = dev_data['main_tweet'].tolist()\n",
    "# dev_encode = tokenizer(dev_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "# dev_label = le.transform(dev_data['label'])\n",
    "# tfmodel.evaluate({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]}, dev_label)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c568c11536c468db92cc2152f72bb087550d740e412a4a332b4e9cdb4eb1d017"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
