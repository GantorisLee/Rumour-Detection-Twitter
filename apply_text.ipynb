{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"According to the New York Times, Warner Bros. wanted to delay Christopher Nolan’s #Tenet months ago due to the COVID-19 pandemic but Nolan refused to listen to any discussions. The studio kept the original release date to “keep [Nolan] happy\"\n",
    "verified = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re, string\n",
    "import keras\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def tokenize_tweet(string_data:str):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = nltk.RegexpTokenizer('\\w+')\n",
    "    data = string_data.replace('\\n', '')\n",
    "    data = data.lower()\n",
    "    data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n",
    "    data = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "    data = tokenized.tokenize(data)\n",
    "    data = [i for i in data if i not in stopwords]\n",
    "    data = [wordnet_lemmatizer.lemmatize(word) for word in data]\n",
    "    data = ' '.join(data)   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 16:55:23.587039: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_file = \"train.data.txt\"\n",
    "train_file = pd.read_csv('./%s.csv'%train_file,keep_default_na=False)\n",
    "train_data = train_file['main_tweet']\n",
    "train_data.replace('', np.nan, inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "train_data = train_data.apply(lambda x: tokenize_tweet(x))\n",
    "vectorizer.fit(train_data.tolist())\n",
    "model = keras.models.load_model(\"tfidf.h5\")\n",
    "# print(text)\n",
    "# print(verified)\n",
    "test_text = tokenize_tweet(text)\n",
    "# print(test_text)\n",
    "test_tfidf = vectorizer.transform([test_text])\n",
    "# print(testtfidf)\n",
    "test_tfidf = scipy.sparse.csr_matrix(test_tfidf).todense()\n",
    "# print(testtfidf)\n",
    "# test = tf.constant([[testtfidf, int(varified)]])\n",
    "# print(test)\n",
    "test_pd = pd.DataFrame(test_tfidf)\n",
    "test_verified = pd.DataFrame({\"verified\":[int(verified)]})\n",
    "test = pd.concat([test_pd, test_verified], axis=1)\n",
    "test = tf.convert_to_tensor(test)\n",
    "# print(test)\n",
    "prediction = model.predict(test)\n",
    "prediction = (prediction > 0.5).astype(\"int32\")\n",
    "prediction = np.ndarray.flatten(prediction)[0]\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6f251bbd1a596d562a3957fc87988337f8e73afd57cc707f1d1bd18f281330c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
