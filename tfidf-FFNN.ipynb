{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = \"train.data.txt\"\n",
    "dev_file = \"dev.data.txt\"\n",
    "test_file = \"test.data.txt\"\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('./%s.csv'%train_file,keep_default_na=False)\n",
    "dev_data = pd.read_csv('./%s.csv'%dev_file,keep_default_na=False)\n",
    "test_data = pd.read_csv('./%s.csv'%test_file,keep_default_na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import scipy\n",
    "import os\n",
    "import keras\n",
    "import string\n",
    "import tensorflow as tf\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "le = LabelEncoder()\n",
    "vectorizer = TfidfVectorizer()\n",
    "def tokenize_tweet(string_data:str):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = nltk.RegexpTokenizer('\\w+')\n",
    "    data = string_data.replace('\\n', '')\n",
    "    data = data.lower()\n",
    "    data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n",
    "    data = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "\n",
    "    data = tokenized.tokenize(data)\n",
    "    data = [i for i in data if i not in stopwords]\n",
    "    data = [wordnet_lemmatizer.lemmatize(word) for word in data]\n",
    "    data = ' '.join(data)   \n",
    "    return data\n",
    "    \n",
    "def preprocess_token(df, dropNa=True):\n",
    "    data = df.copy()\n",
    "    data['main_tweet'] = data['main_tweet'].fillna('')\n",
    "    if dropNa:\n",
    "        data.replace('', np.nan, inplace=True)\n",
    "        data.dropna(subset=['main_tweet'], inplace=True)\n",
    "    data['main_tweet'] = data['main_tweet'].apply(lambda x: tokenize_tweet(x))\n",
    "    data['verified'] = data['verified'].astype(int)\n",
    "    return data\n",
    "\"\"\"\n",
    "    Preprocessing the training data\n",
    "\"\"\"\n",
    "\n",
    "train = preprocess_token(train_data)\n",
    "traintfidf = vectorizer.fit_transform(train['main_tweet'].tolist())\n",
    "traintfidf = scipy.sparse.csr_matrix(traintfidf).todense()\n",
    "d = pd.DataFrame(traintfidf)\n",
    "verified = pd.DataFrame({\"verified\":train['verified'].tolist()})\n",
    "followers = pd.DataFrame({\"followers\":train['followers'].tolist()})\n",
    "train_label = le.fit_transform(train['label'].tolist())\n",
    "train = pd.concat([d, verified], axis=1)\n",
    "train = tf.convert_to_tensor(train)\n",
    "\n",
    "\"\"\"\n",
    "    Preprocessing the dev data\n",
    "\"\"\"\n",
    "dev = preprocess_token(dev_data)\n",
    "devtfidf = vectorizer.transform(dev['main_tweet'].tolist())\n",
    "devtfidf = scipy.sparse.csr_matrix(devtfidf).todense()\n",
    "devpd = pd.DataFrame(devtfidf)\n",
    "dev_label=le.transform(dev['label'].tolist())\n",
    "dev_verified = pd.DataFrame({\"verified\":dev['verified'].tolist()})\n",
    "dev_followers = pd.DataFrame({\"followers\":dev['followers'].tolist()})\n",
    "dev = pd.concat([devpd, dev_verified], axis=1)\n",
    "dev = tf.convert_to_tensor(dev)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Preprocessing the test data\n",
    "\"\"\"\n",
    "test = preprocess_token(test_data, dropNa=False)\n",
    "testtfidf = vectorizer.transform(test['main_tweet'].tolist())\n",
    "testtfidf = scipy.sparse.csr_matrix(testtfidf).todense()\n",
    "testpd = pd.DataFrame(testtfidf)\n",
    "test_verified = pd.DataFrame({\"verified\":test['verified'].tolist()})\n",
    "test_followers = pd.DataFrame({\"followers\":test['followers'].tolist()})\n",
    "test = pd.concat([testpd, test_verified], axis=1)\n",
    "test = tf.convert_to_tensor(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new coronaviru', 'prevent treat', 'herd immun', 'disea', 'herd', 'symptom', 'treat', 'effect', 'transmit', 'infect new']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "746576"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import scattertext as st\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "corpus = st.CorpusFromPandas(train_data,\n",
    "                             category_col='label',\n",
    "                             text_col='main_tweet',\n",
    "\n",
    "                             nlp=nlp).build()\n",
    "term_freq_df = corpus.get_term_freq_df()\n",
    "term_freq_df['nonrumour score'] = corpus.get_scaled_f_scores('nonrumour')\n",
    "\n",
    "print(list(term_freq_df.sort_values(by='nonrumour score', ascending=False).index[:10]))\n",
    "train_data.iloc[0]\n",
    "html = st.produce_scattertext_explorer(corpus,\\\n",
    "        category='nonrumour',\\\n",
    "        category_name='Nonrumour',\\\n",
    "        not_category_name='Rumour',\\\n",
    "        width_in_pixels=1000,\\\n",
    "        metadata=train_data['label'])\n",
    "open(\"./Convention-Visualization.html\", 'wb').write(html.encode('utf-8'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new coronaviru affect older peopl younger peopl also suscept health'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_tweet(\"Does the new coronavirus affect older people, or are younger people also susceptible? #Covid_19 #health https://t.co/Kg0Qb5oMs8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 128)               665728    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 676,097\n",
      "Trainable params: 676,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.3885 - accuracy: 0.8168 - val_loss: 0.2773 - val_accuracy: 0.9009\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 0.0734 - accuracy: 0.9872 - val_loss: 0.2361 - val_accuracy: 0.9159\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2907 - val_accuracy: 0.9140\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 5.5232e-04 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.9140\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 2.8609e-04 - accuracy: 1.0000 - val_loss: 0.3176 - val_accuracy: 0.9140\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.7800e-04 - accuracy: 1.0000 - val_loss: 0.3275 - val_accuracy: 0.9159\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 1.2032e-04 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.9178\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 8.6083e-05 - accuracy: 1.0000 - val_loss: 0.3425 - val_accuracy: 0.9178\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 6.3908e-05 - accuracy: 1.0000 - val_loss: 0.3488 - val_accuracy: 0.9178\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 4.8829e-05 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.9178\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 3.8272e-05 - accuracy: 1.0000 - val_loss: 0.3609 - val_accuracy: 0.9178\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 3.0492e-05 - accuracy: 1.0000 - val_loss: 0.3657 - val_accuracy: 0.9178\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 2.4748e-05 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.9178\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 2.0333e-05 - accuracy: 1.0000 - val_loss: 0.3748 - val_accuracy: 0.9140\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 1.6841e-05 - accuracy: 1.0000 - val_loss: 0.3788 - val_accuracy: 0.9140\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4061e-05 - accuracy: 1.0000 - val_loss: 0.3832 - val_accuracy: 0.9140\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.1848e-05 - accuracy: 1.0000 - val_loss: 0.3867 - val_accuracy: 0.9140\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.0062e-05 - accuracy: 1.0000 - val_loss: 0.3905 - val_accuracy: 0.9140\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 8.6170e-06 - accuracy: 1.0000 - val_loss: 0.3938 - val_accuracy: 0.9140\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 7.4191e-06 - accuracy: 1.0000 - val_loss: 0.3973 - val_accuracy: 0.9140\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',input_shape=(train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train, train_label,epochs=20, validation_data=(dev, dev_label) ,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test)\n",
    "prediction = (prediction > 0.5).astype(\"int32\")\n",
    "prediction = np.ndarray.flatten(prediction)\n",
    "pd.DataFrame({\"Predicted\":  prediction}).to_csv('submission.csv', index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def update_data(df, filename):\n",
    "    copy_test = df.copy()\n",
    "    data_empty_test = copy_test[copy_test['main_tweet'].isna()]\n",
    "    listid = data_empty_test['main_tweet_id'].tolist()\n",
    "    print(len(listid))\n",
    "    i = 0\n",
    "    for id in listid:\n",
    "        file = './project-data/tweet-objects/tweet-objects/%s.json'%id\n",
    "        if os.path.exists(file):\n",
    "            with open(file, 'r') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                copy_test.loc[copy_test['main_tweet_id'] == id, 'verified'] = data['user']['verified']\n",
    "                copy_test.loc[copy_test['main_tweet_id'] == id, 'followers'] = data['user']['followers_count']\n",
    "                copy_test.loc[copy_test['main_tweet_id'] == id, 'main_tweet'] = data['text']\n",
    "                i+=1\n",
    "\n",
    "    copy_test.to_csv(filename, index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328\n",
      "96\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "t2 = pd.read_csv('./project-data/train.data.txt.csv')\n",
    "t3 = pd.read_csv('./project-data/dev.data.txt.csv')\n",
    "t4 = pd.read_csv('./project-data/test.data.txt.csv')\n",
    "\n",
    "\n",
    "label = open('./project-data/train.label.txt', 'r')\n",
    "label_data = label.readlines()\n",
    "label_data = [x.strip() for x in label_data]\n",
    "t2['label'] = label_data\n",
    "\n",
    "label = open('./project-data/dev.label.txt', 'r')\n",
    "label_data = label.readlines()\n",
    "label_data = [x.strip() for x in label_data]\n",
    "t3['label'] = label_data\n",
    "\n",
    "\n",
    "label_data = label.readlines()\n",
    "update_data(t2, './project-data/train.data.txt.csv')\n",
    "update_data(t3, './project-data/dev.data.txt.csv')\n",
    "update_data(t4, './project-data/test.data.txt.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cdb69ebf63b127ccdd1f4d2b9e6e004bc1933417c744d4c5778c2e874cc47f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('pydml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
