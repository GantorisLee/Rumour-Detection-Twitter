{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"covid.data.txt\"\n",
    "\n",
    "tokens = [\"AAAAAAAAAAAAAAAAAAAAAIHFbAEAAAAA0oBVG5orLLErnyAqw2po3fOae5w%3D4lgZWoMOyGG496F2aNACoKOdDCaDnxqret6oFPLToE244O6Tx6\",\n",
    "          \"AAAAAAAAAAAAAAAAAAAAABoebgEAAAAA8nOwpOH6GviG0pVFbGAEowC6lrE%3DvQmBGiPwQ2EJKcoMQUNYDwgb0d5vkjxvSPtkuqY0FFGY9D2yS8\",\n",
    "          \"AAAAAAAAAAAAAAAAAAAAAGfpbgEAAAAAdDlnOAJp18hBJE2XvpIFR5BT%2FlU%3Dxx6v1V8A01WYe5sol5PM7o6RF88eKMxoB5uocVXJr4U7K2Ngkr\",\n",
    "          \"AAAAAAAAAAAAAAAAAAAAAEgxbwEAAAAANpQeNdE%2FBiaB9RncHdfQS3a0dsI%3DfcpHpWk2XSl9zhP6tknT3PzFbWSeWiUY7LJQMM3MxhcF0g9MPf\",\n",
    "          \"AAAAAAAAAAAAAAAAAAAAACkXbwEAAAAA4EaRYdGe0XQmMdEz1BmhgY0W7bY%3D8TyRvFFaa8f9ti7HSiKRkwTkQMXQwNrCOZLtLVbbmEMgLVcXlb\",\n",
    "          \"AAAAAAAAAAAAAAAAAAAAANe%2FbAEAAAAA9jYx0Inaeh%2FbvLfz%2FdHmeN8PRXQ%3DhlJ0qUNoA5Bss5eStWnhRniTUgF2Z4AQhbcc1nrS3wEWA6Petn\"\n",
    "        ]\n",
    "limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from pytz import UTC\n",
    "import scipy\n",
    "from tweepy import Client\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jiachenli/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jiachenli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_step():\n",
    "    time_step = []\n",
    "    for m in [1, 3, 6, 9]:\n",
    "        time_step.append(datetime(2020, m, 1, hour=0,minute=0, tzinfo=UTC))\n",
    "    time_step.append(datetime(2021, 1, 1, hour=0,minute=0, tzinfo=UTC))\n",
    "    return time_step\n",
    "\n",
    "time_step = get_time_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_tweet = \"New Harris County #Houston #COVID19 numbers online tonight @HoustonChron. Forget the 2 week grace period you heard this AM at the House Energy Commerce testimony (which didn't make sense to me), we're at this point now, unfortunately.  Discussing tomorrow @NewDay @CNN early start https://t.co/oAqEbXbsEC\"\n",
    "def hash_tags_analysis(text):\n",
    "    tags = {tag.strip(\"#\") for tag in text.split() if tag.startswith(\"#\")}\n",
    "    return tags\n",
    "# print(hash_tags_analysis(fake_tweet))\n",
    "\n",
    "def at_analysis(text):\n",
    "    tags = {tag.strip(\"@\") for tag in text.split() if tag.startswith(\"@\")}\n",
    "    return tags    \n",
    "# print(at_analysis(fake_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text1 = '''Hilarious ðŸ˜‚. The feeling of making a sale ðŸ˜Ž, The feeling of actually fulfilling orders ðŸ˜’'''\n",
    "def sentiment_analysis(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    def convert_emojis(text):\n",
    "        for emot in UNICODE_EMOJI:\n",
    "            text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        return text.replace(\"_\",\" \")\n",
    "    text = convert_emojis(text)\n",
    "    return sia.polarity_scores(text)\n",
    "# print(sentiment_analysis(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_slot(test_date):\n",
    "    for i in range(0,len(time_step) - 1):\n",
    "        if test_date >= time_step[i] and test_date<=time_step[i+1]:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tweet(string_data:str):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = nltk.RegexpTokenizer('\\w+')\n",
    "    data = string_data.replace('\\n', '')\n",
    "    data = data.lower()\n",
    "    data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n",
    "    data = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "    data = tokenized.tokenize(data)\n",
    "    data = [i for i in data if i not in stopwords]\n",
    "    data = [wordnet_lemmatizer.lemmatize(word) for word in data]\n",
    "    data = ' '.join(data)   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [{}, {}]\n",
    "hashtags = [{}, {}]\n",
    "sentiment = [dict({'neg': [], 'neu': [], 'pos': [], 'compound': []}),\n",
    "             dict({'neg': [], 'neu': [], 'pos': [], 'compound': []})]\n",
    "public_metrics = [dict({'followers_count': [], 'following_count': [], 'tweet_count': [], 'listed_count': []}),\n",
    "                  dict({'followers_count': [], 'following_count': [], 'tweet_count': [], 'listed_count': []})]\n",
    "count = [0, 0]\n",
    "count_time = [0,0,0,0]\n",
    "ats = [{}, {}]\n",
    "topic_time = [{},{},{},{}]\n",
    "verified_count = [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet(id, model, vectorizer, client):\n",
    "    # client = Client(BEARER_TOKEN, wait_on_rate_limit=True)\n",
    "    tweet = client.get_tweet(id, expansions = [\"author_id\"],tweet_fields=[\"context_annotations\", \"created_at\",\"entities\",\"public_metrics\"], user_fields=[\"verified\", \"public_metrics\"])\n",
    "    if tweet.data == None:\n",
    "        return\n",
    "    text = tweet.data.text\n",
    "    # print(text)\n",
    "    verified = tweet.includes[\"users\"][0].verified\n",
    "    # print(verified)\n",
    "    test_text = tokenize_tweet(text)\n",
    "    # print(test_text)\n",
    "    test_tfidf = vectorizer.transform([test_text])\n",
    "    # print(testtfidf)\n",
    "    test_tfidf = scipy.sparse.csr_matrix(test_tfidf).todense()\n",
    "    # print(testtfidf)\n",
    "    # test = tf.constant([[testtfidf, int(varified)]])\n",
    "    # print(test)\n",
    "    test_pd = pd.DataFrame(test_tfidf)\n",
    "    test_verified = pd.DataFrame({\"verified\":[int(verified)]})\n",
    "    test = pd.concat([test_pd, test_verified], axis=1)\n",
    "    test = tf.convert_to_tensor(test)\n",
    "    # print(test)\n",
    "    prediction = model.predict(test)\n",
    "    prediction = (prediction > 0.5).astype(\"int32\")\n",
    "    prediction = np.ndarray.flatten(prediction)[0]\n",
    "    # print(prediction)\n",
    "    # 0 is non rumour \n",
    "    count[prediction] += 1\n",
    "    \n",
    "    # print(tweet.data.context_annotations)\n",
    "    # task 1 \n",
    "    # What are the topics of COVID-19 rumours, and how do they differ from the non-rumours?\n",
    "    domains = {d[\"domain\"][\"name\"] for d in tweet.data.context_annotations}\n",
    "    \n",
    "    for d in domains:\n",
    "        topics[prediction][d] = topics[prediction].get(d, 0) + 1\n",
    "\n",
    "    # print(domains)\n",
    "    # task 2 \n",
    "    # How do COVID-19 rumour topics or trends evolve over time?\n",
    "    if prediction:\n",
    "        sent_date = tweet.data.created_at\n",
    "        slot = get_time_slot(sent_date)\n",
    "        count_time[slot] += 1\n",
    "        for d in domains:\n",
    "            topic_time[slot][d] = topic_time[slot].get(d, 0) + 1\n",
    "    # print(sent_date)\n",
    "    # task 3\n",
    "    # What are the popular hashtags of COVID-19 rumours and non-rumours? How much overlap or difference do they share?\n",
    "    tags = hash_tags_analysis(text)\n",
    "    for t in tags:\n",
    "        hashtags[prediction][t] = hashtags[prediction].get(t, 0) + 1\n",
    "    # task 4\n",
    "    # Do rumour source tweets convey a different sentiment/emotion to the non-rumour source tweets? What about their replies? \n",
    "    sentiment_score = sentiment_analysis(text)\n",
    "    # sentiment[prediction] = {sentiment[prediction][k].append(sentiment_score[k]) for k in sentiment_score.keys()}\n",
    "    \n",
    "    for k in sentiment_score.keys():\n",
    "        sentiment[prediction][k] += [sentiment_score[k]]\n",
    "    # task 5\n",
    "    # What are the characteristics of rumour-creating users, and are they different to normal users?\n",
    "    \n",
    "    pm = tweet.includes[\"users\"][0].public_metrics\n",
    "    # public_metrics[prediction] = {public_metrics[prediction][k] + [pm[k]] for k in pm.keys()}\n",
    "    for k in pm.keys():\n",
    "        public_metrics[prediction][k] += [pm[k]]\n",
    "    \n",
    "    # extra 1 at analysis\n",
    "    # What are the characteristics of rumour-creating users, and are they different to normal users?\n",
    "    \n",
    "    tweet_ats = at_analysis(text)\n",
    "    for a in tweet_ats:\n",
    "        ats[prediction][a] = ats[prediction].get(a, 0) + 1\n",
    "\n",
    "    if verified:\n",
    "        verified_count[prediction] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_file = \"train.data.txt\"\n",
    "train_file = pd.read_csv('./%s.csv'%train_file,keep_default_na=False)\n",
    "train_data = train_file['main_tweet']\n",
    "train_data.replace('', np.nan, inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "train_data = train_data.apply(lambda x: tokenize_tweet(x))\n",
    "vectorizer.fit(train_data.tolist())\n",
    "model = keras.models.load_model(\"tfidf.h5\")\n",
    "file_open = open(file, 'r')\n",
    "\n",
    "clients = [Client(t, wait_on_rate_limit=True) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 17:46:26.935688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "counter = 0 \n",
    "for line in file_open.readlines():\n",
    "    line = line.strip()\n",
    "    ids = line.split(',')\n",
    "    source_id = ids[0]\n",
    "    client = clients[counter%len(clients)]\n",
    "    get_tweet(source_id, model, vectorizer, client)\n",
    "    print(counter)\n",
    "    counter += 1\n",
    "    if limit != 0 and counter > limit:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic from 2020-01-01 00:00:00+00:00 to 2020-03-01 00:00:00+00:00 are \n",
      " {}\n",
      "\n",
      "Topic from 2020-03-01 00:00:00+00:00 to 2020-06-01 00:00:00+00:00 are \n",
      " {}\n",
      "\n",
      "Topic from 2020-06-01 00:00:00+00:00 to 2020-09-01 00:00:00+00:00 are \n",
      " {'Ongoing News Story': 2, 'Events [Entity Service]': 1, 'Politician': 1, 'Person': 1}\n",
      "\n",
      "Topic from 2020-09-01 00:00:00+00:00 to 2021-01-01 00:00:00+00:00 are \n",
      " {}\n",
      "\n",
      "The topics for nonrumour are \n",
      " {'Movie': 1, 'Person': 1, 'Ongoing News Story': 3, 'Events [Entity Service]': 2, 'Brand': 2, 'Movie Genre': 1}\n",
      "\n",
      "The topics for rumour are \n",
      " {'Ongoing News Story': 2, 'Events [Entity Service]': 1, 'Politician': 1, 'Person': 1}\n",
      "\n",
      "The hashtags for nonrumour are \n",
      " {'Tenet': 1, 'COVID19': 1}\n",
      "\n",
      "The hashtags for rumour are \n",
      " {}\n",
      "\n",
      "The sentiment for nonrumour are \n",
      " {'neg': [0.098, 0.333, 0.17], 'neu': [0.837, 0.667, 0.716], 'pos': [0.065, 0.0, 0.114], 'compound': [-0.128, -0.6705, -0.3254]}\n",
      "\n",
      "The sentiment for rumour are \n",
      " {'neg': [0.0, 0.145], 'neu': [1.0, 0.725], 'pos': [0.0, 0.13], 'compound': [0.0, -0.1531]}\n",
      "\n",
      "The public_metrics for nonrumour are \n",
      " {'followers_count': [44964, 4367570, 1179769], 'following_count': [16, 298, 488], 'tweet_count': [5667, 1015279, 256709], 'listed_count': [274, 29069, 6320]}\n",
      "\n",
      "The public_metrics for rumour are \n",
      " {'followers_count': [297565, 204272], 'following_count': [56363, 1378], 'tweet_count': [111050, 19783], 'listed_count': [5495, 2065]}\n",
      "\n",
      "The count for nonrumour are \n",
      " 3\n",
      "\n",
      "The count for rumour are \n",
      " 2\n",
      "\n",
      "The ats for nonrumour are \n",
      " {}\n",
      "\n",
      "The ats for rumour are \n",
      " {}\n",
      "\n",
      "The verified_count for nonrumour are \n",
      " 2\n",
      "\n",
      "The verified_count for rumour are \n",
      " 2\n",
      "\n",
      "                         nonrumour  rumour\n",
      "Movie                            1       0\n",
      "Politician                       0       1\n",
      "Person                           1       1\n",
      "Ongoing News Story               3       2\n",
      "Events [Entity Service]          2       1\n",
      "Brand                            2       0\n",
      "Movie Genre                      1       0\n",
      "         nonrumour  rumour\n",
      "COVID19          1       0\n",
      "Tenet            1       0\n",
      "Empty DataFrame\n",
      "Columns: [nonrumour, rumour]\n",
      "Index: []\n",
      "                         2020/1/1-2020/3/1  2020/3/1-2020/6/1  \\\n",
      "Politician                               0                  0   \n",
      "Ongoing News Story                       0                  0   \n",
      "Events [Entity Service]                  0                  0   \n",
      "Person                                   0                  0   \n",
      "\n",
      "                         2020/6/1-2020/9/1  2020/9/1-2021/1/1  \n",
      "Politician                               1                  0  \n",
      "Ongoing News Story                       2                  0  \n",
      "Events [Entity Service]                  1                  0  \n",
      "Person                                   1                  0  \n",
      "     neg    neu    pos  compound\n",
      "0  0.098  0.837  0.065   -0.1280\n",
      "1  0.333  0.667  0.000   -0.6705\n",
      "2  0.170  0.716  0.114   -0.3254\n",
      "     neg    neu   pos  compound\n",
      "0  0.000  1.000  0.00    0.0000\n",
      "1  0.145  0.725  0.13   -0.1531\n",
      "   followers_count  following_count  tweet_count  listed_count\n",
      "0            44964               16         5667           274\n",
      "1          4367570              298      1015279         29069\n",
      "2          1179769              488       256709          6320\n",
      "   followers_count  following_count  tweet_count  listed_count\n",
      "0           297565            56363       111050          5495\n",
      "1           204272             1378        19783          2065\n"
     ]
    }
   ],
   "source": [
    "# printing out result for trend\n",
    "for i in range(0, len(topic_time)):\n",
    "    print(f\"Topic from {time_step[i]} to {time_step[i+1]} are \\n {topic_time[i]}\\n\")\n",
    "    \n",
    "    \n",
    "metrics = [topics, hashtags, sentiment, public_metrics, count, ats, verified_count]\n",
    "metric_names = [\"topics\", \"hashtags\", \"sentiment\", \"public_metrics\", \"count\", \"ats\", \"verified_count\"]\n",
    "for k in range(0,len(metrics)):\n",
    "    for p in [0,1]:\n",
    "        clf = \"rumour\" if p else \"nonrumour\"\n",
    "        print(f\"The {metric_names[k]} for {clf} are \\n {metrics[k][p]}\")\n",
    "        print()\n",
    "    \n",
    " # Output topics, hashtags, and ats to files\n",
    "tasks = [topics, hashtags, ats]\n",
    "file_name = [\"topics\", \"hashtags\", \"ats\"]\n",
    "label = [\"topic\", \"hashtag\", \"user\"]\n",
    "num = 0\n",
    "for task in tasks:\n",
    "    task_index = list(set(list(task[0].keys()) + list(task[1].keys())))\n",
    "    task_dict = {\"nonrumour\":[], \"rumour\": []}\n",
    "    for i in task_index:\n",
    "        task_dict[\"nonrumour\"] = task_dict[\"nonrumour\"] + [task[0].get(i, 0)]\n",
    "        task_dict[\"rumour\"] = task_dict[\"rumour\"] + [task[1].get(i, 0)]\n",
    "    task_df = pd.DataFrame(data=task_dict, index = task_index)\n",
    "    print(task_df)\n",
    "    task_df.to_csv(f\"output/{file_name[num]}.csv\",index=True, index_label=label[num])\n",
    "    num += 1\n",
    "    \n",
    "# Output trend to files\n",
    "trend_index = []\n",
    "for d in topic_time:\n",
    "    trend_index = trend_index + list(d.keys())\n",
    "trend_index = list(set(trend_index))\n",
    "trend_dict = {\"2020/1/1-2020/3/1\":[], \"2020/3/1-2020/6/1\": [], \"2020/6/1-2020/9/1\": [], \n",
    "              \"2020/9/1-2021/1/1\": []}\n",
    "for i in trend_index:\n",
    "    foo = 0\n",
    "    for k in trend_dict.keys():\n",
    "        trend_dict[k] = trend_dict[k] + [topic_time[foo].get(i, 0)]\n",
    "        foo += 1\n",
    "trend_df = pd.DataFrame(data=trend_dict, index = trend_index)\n",
    "print(trend_df)\n",
    "trend_df.to_csv(\"output/trend.csv\",index=True, index_label=\"Topic\")\n",
    "    \n",
    "# sentiment\n",
    "sentiment_nonrumour = pd.DataFrame(data=sentiment[0])\n",
    "sentiment_rumour = pd.DataFrame(data=sentiment[1])\n",
    "print(sentiment_nonrumour)\n",
    "print(sentiment_rumour)\n",
    "sentiment_nonrumour.to_csv(\"output/sentiment_nonrumour.csv\",index=False)\n",
    "sentiment_rumour.to_csv(\"output/sentiment_rumour.csv\",index=False)\n",
    "# metrics \n",
    "public_metrics_nonrumour = pd.DataFrame(data=public_metrics[0])\n",
    "public_metrics_rumour = pd.DataFrame(data=public_metrics[1])\n",
    "print(public_metrics_nonrumour)\n",
    "print(public_metrics_rumour)\n",
    "public_metrics_nonrumour.to_csv(\"output/public_metrics_nonrumour.csv\",index=False)\n",
    "public_metrics_rumour.to_csv(\"output/public_metrics_rumour.csv\",index=False)\n",
    "    \n",
    "count_dict = {\"nonrumour\": count[0], \"rumour\": count[1],\n",
    "               \"nonrumour_verified\": verified_count[0], \n",
    "               \"rumour_verified\": verified_count[1],\n",
    "               \"2020/1/1-2020/3/1\":count_time[0], \n",
    "               \"2020/3/1-2020/6/1\": count_time[1], \n",
    "               \"2020/6/1-2020/9/1\": count_time[2], \n",
    "               \"2020/9/1-2021/1/1\": count_time[3]}\n",
    "count_df = pd.DataFrame(data = count_dict, index=[0])\n",
    "count_df.to_csv(\"output/count.csv\", index=True, index_label=\"0\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6f251bbd1a596d562a3957fc87988337f8e73afd57cc707f1d1bd18f281330c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
