{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guoyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\guoyi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_tweet</th>\n",
       "      <th>main_tweet_id</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers</th>\n",
       "      <th>replies</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 regularly rinsing nose saline help prevent i...</td>\n",
       "      <td>1250219300389974016</td>\n",
       "      <td>False</td>\n",
       "      <td>410</td>\n",
       "      <td>[{\"tweet_id\": 1250219116993974272, \"tweet\": \"4...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french police chief killed charliehebdo attack</td>\n",
       "      <td>554886875303780352</td>\n",
       "      <td>True</td>\n",
       "      <td>3229894</td>\n",
       "      <td>[{\"tweet_id\": 554959644125167617, \"tweet\": \"De...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus disease covid19 advice public\\r\\r‚ú≥...</td>\n",
       "      <td>1237901309011021825</td>\n",
       "      <td>False</td>\n",
       "      <td>613</td>\n",
       "      <td>[{\"tweet_id\": 1237901311439450112, \"tweet\": \"I...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ottawa police confirm multiple suspects shooti...</td>\n",
       "      <td>524958128392376320</td>\n",
       "      <td>True</td>\n",
       "      <td>19783124</td>\n",
       "      <td>[{\"tweet_id\": 524961934064754688, \"tweet\": \"@W...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>primary focus government isnt alleviate suffer...</td>\n",
       "      <td>1239295488677085185</td>\n",
       "      <td>False</td>\n",
       "      <td>4889</td>\n",
       "      <td>[]</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>4 cannot transmitted goods manufactured china ...</td>\n",
       "      <td>1237545128828342277</td>\n",
       "      <td>False</td>\n",
       "      <td>631</td>\n",
       "      <td>[{\"tweet_id\": 1237545126278258703, \"tweet\": \"#...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>desperate ted cruz claims planned parenthood s...</td>\n",
       "      <td>671181758692507648</td>\n",
       "      <td>True</td>\n",
       "      <td>143090</td>\n",
       "      <td>[{\"tweet_id\": 671200376843067392, \"tweet\": \"@B...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>thoughts prayers enough pres obama speaks mass...</td>\n",
       "      <td>672513234419638273</td>\n",
       "      <td>True</td>\n",
       "      <td>17449031</td>\n",
       "      <td>[{\"tweet_id\": 672513853645717504, \"tweet\": \"@A...</td>\n",
       "      <td>rumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>police surrounded building suspected charliehe...</td>\n",
       "      <td>553508098825261056</td>\n",
       "      <td>True</td>\n",
       "      <td>9077962</td>\n",
       "      <td>[{\"tweet_id\": 553509546602553344, \"tweet\": \"@N...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>lynnesimpkin help üë©‚Äçüè´\\r9am socialism  need bad...</td>\n",
       "      <td>1241082793737818113</td>\n",
       "      <td>False</td>\n",
       "      <td>3103</td>\n",
       "      <td>[{\"tweet_id\": 1241041450084839424, \"tweet\": \"E...</td>\n",
       "      <td>nonrumour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             main_tweet        main_tweet_id  \\\n",
       "0     5 regularly rinsing nose saline help prevent i...  1250219300389974016   \n",
       "1      french police chief killed charliehebdo attack     554886875303780352   \n",
       "2     coronavirus disease covid19 advice public\\r\\r‚ú≥...  1237901309011021825   \n",
       "3     ottawa police confirm multiple suspects shooti...   524958128392376320   \n",
       "4     primary focus government isnt alleviate suffer...  1239295488677085185   \n",
       "...                                                 ...                  ...   \n",
       "1889  4 cannot transmitted goods manufactured china ...  1237545128828342277   \n",
       "1890  desperate ted cruz claims planned parenthood s...   671181758692507648   \n",
       "1891  thoughts prayers enough pres obama speaks mass...   672513234419638273   \n",
       "1892  police surrounded building suspected charliehe...   553508098825261056   \n",
       "1894  lynnesimpkin help üë©‚Äçüè´\\r9am socialism  need bad...  1241082793737818113   \n",
       "\n",
       "      verified  followers                                            replies  \\\n",
       "0        False        410  [{\"tweet_id\": 1250219116993974272, \"tweet\": \"4...   \n",
       "1         True    3229894  [{\"tweet_id\": 554959644125167617, \"tweet\": \"De...   \n",
       "2        False        613  [{\"tweet_id\": 1237901311439450112, \"tweet\": \"I...   \n",
       "3         True   19783124  [{\"tweet_id\": 524961934064754688, \"tweet\": \"@W...   \n",
       "4        False       4889                                                 []   \n",
       "...        ...        ...                                                ...   \n",
       "1889     False        631  [{\"tweet_id\": 1237545126278258703, \"tweet\": \"#...   \n",
       "1890      True     143090  [{\"tweet_id\": 671200376843067392, \"tweet\": \"@B...   \n",
       "1891      True   17449031  [{\"tweet_id\": 672513853645717504, \"tweet\": \"@A...   \n",
       "1892      True    9077962  [{\"tweet_id\": 553509546602553344, \"tweet\": \"@N...   \n",
       "1894     False       3103  [{\"tweet_id\": 1241041450084839424, \"tweet\": \"E...   \n",
       "\n",
       "          label  \n",
       "0     nonrumour  \n",
       "1        rumour  \n",
       "2     nonrumour  \n",
       "3     nonrumour  \n",
       "4     nonrumour  \n",
       "...         ...  \n",
       "1889  nonrumour  \n",
       "1890     rumour  \n",
       "1891     rumour  \n",
       "1892  nonrumour  \n",
       "1894  nonrumour  \n",
       "\n",
       "[1567 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "train_file = \"train.data.txt\"\n",
    "dev_file = \"dev.data.txt\"\n",
    "test_file = \"test.data.txt\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def tokenize_tweet(string_data:str):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = nltk.RegexpTokenizer('\\w+')\n",
    "    data = string_data.replace('\\n', '')\n",
    "    data = data.lower()\n",
    "    data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n",
    "    data = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "    # data = [wordnet_lemmatizer.lemmatize(word) for word in data.split(' ')]\n",
    "    data = ' '.join([i for i in data.split(' ') if i not in stopwords])\n",
    "    return data\n",
    "\n",
    "train_data = pd.read_csv('./%s.csv'%train_file,keep_default_na=False)\n",
    "dev_data = pd.read_csv('./%s.csv'%dev_file,keep_default_na=False)\n",
    "test_data = pd.read_csv('./%s.csv'%test_file,keep_default_na=False)\n",
    "def preprocess_token(df, dropNa=True):\n",
    "    data = df.copy()\n",
    "    data['main_tweet'] = data['main_tweet'].fillna('')\n",
    "    if dropNa:\n",
    "        data.replace('', np.nan, inplace=True)\n",
    "        data.dropna(subset=['main_tweet'], inplace=True)\n",
    "    text = data['main_tweet'].apply(lambda x: tokenize_tweet(x))\n",
    "    data['main_tweet'] = text\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data=preprocess_token(train_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (4.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: requests in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\guoyi\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_label = le.fit_transform(train_data['label'])\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,do_lower_case=True)\n",
    "train_text = train_data['main_tweet'].tolist()\n",
    "encoding = tokenizer(train_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "dev_data=preprocess_token(dev_data)\n",
    "dev_text = dev_data['main_tweet'].tolist()\n",
    "dev_encode = tokenizer(dev_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "dev_label = le.transform(dev_data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1567, 100)\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "print(encoding[\"input_ids\"].shape)\n",
    "\n",
    "def create_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(100,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(100,), name='attention_mask', dtype='int32')\n",
    "\n",
    "    embeddings = model(input_ids, attention_mask=mask)[0]\n",
    "    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "    X = tf.keras.layers.Dropout(0.1)(X)\n",
    "    y = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(X)\n",
    "\n",
    "    tfmodel = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "    return tfmodel\n",
    "\n",
    "tfmodel = create_model()\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "# optimizer = tf.keras.optimizers.Adam(2e-5)\n",
    "tfmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "history = tfmodel.fit({\"input_ids\": encoding[\"input_ids\"], 'attention_mask': encoding[\"attention_mask\"]}, \\\n",
    "                      train_label,\\\n",
    "                      validation_data=({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]}, dev_label),\\\n",
    "                      epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfmodel.save(\"model3.h5\")\n",
    "\n",
    "# dev_data=preprocess_token(dev_data)\n",
    "\n",
    "# dev_text = dev_data['main_tweet'].tolist()\n",
    "# dev_encode = tokenizer(dev_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "# dev_label = le.transform(dev_data['label'])\n",
    "\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(2e-5)\n",
    "# tfmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# tfmodel.evaluate({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]}, dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data=preprocess_token(dev_data)\n",
    "\n",
    "dev_text = dev_data['main_tweet'].tolist()\n",
    "dev_encode = tokenizer(dev_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")\n",
    "dev_label = le.transform(dev_data['label'])\n",
    "\n",
    "test_data=preprocess_token(test_data)\n",
    "\n",
    "test_text = test_data['main_tweet'].tolist()\n",
    "test_encode = tokenizer(test_text,truncation=True,max_length=100, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 44s 2s/step - loss: 0.1660 - accuracy: 0.9402\n"
     ]
    }
   ],
   "source": [
    "# #### To run for Bert+2e-5\n",
    "\n",
    "# tfmodel.load_weights('bert-weight.h5')\n",
    "# optimizer = tf.keras.optimizers.Adam(2e-5)\n",
    "# tfmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# tfmodel.evaluate({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]}, dev_label)\n",
    "\n",
    "# ##create Bert for test 2e5\n",
    "# prediction = tfmodel.predict({\"input_ids\": test_encode[\"input_ids\"], 'attention_mask': test_encode[\"attention_mask\"]})\n",
    "# prediction = (prediction > 0.5).astype(\"int32\")\n",
    "# prediction = np.ndarray.flatten(prediction)\n",
    "# pd.DataFrame({\"Predicted\":  prediction}).to_csv('Bert+2e5-submit.csv', index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 100,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 1)            129         ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,583,873\n",
      "Trainable params: 109,582,337\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tfmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert+2e5 on dev set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9663    0.9571    0.9617       420\n",
      "           1     0.8487    0.8783    0.8632       115\n",
      "\n",
      "    accuracy                         0.9402       535\n",
      "   macro avg     0.9075    0.9177    0.9125       535\n",
      "weighted avg     0.9411    0.9402    0.9406       535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create Bert for 2e5 for dev\n",
    "# predictionxx = tfmodel.predict({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]})\n",
    "# predictionxx = (predictionxx > 0.5).astype(\"int32\")\n",
    "# predictionxx = np.ndarray.flatten(predictionxx)\n",
    "# print(\"Bert+2e5 on dev set\")\n",
    "# print(classification_report(dev_label, predictionxx, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 44s 2s/step - loss: 0.1720 - accuracy: 0.9533\n"
     ]
    }
   ],
   "source": [
    "#### To run for Bert+3e-5\n",
    "\n",
    "# tfmodel.load_weights('model3.h5')\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "tfmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "tfmodel.evaluate({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]}, dev_label)\n",
    "\n",
    "##create Bert for test 3e5\n",
    "predictionz = tfmodel.predict({\"input_ids\": test_encode[\"input_ids\"], 'attention_mask': test_encode[\"attention_mask\"]})\n",
    "predictionz = (predictionz > 0.5).astype(\"int32\")\n",
    "predictionz = np.ndarray.flatten(predictionz)\n",
    "pd.DataFrame({\"Predicted\":  predictionz}).to_csv('Bert+3e5-submit.csv', index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert+3e5 on dev set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9669    0.9738    0.9703       420\n",
      "           1     0.9018    0.8783    0.8899       115\n",
      "\n",
      "    accuracy                         0.9533       535\n",
      "   macro avg     0.9343    0.9260    0.9301       535\n",
      "weighted avg     0.9529    0.9533    0.9530       535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #create Bert for 2e5 for dev\n",
    "# predictionzz = tfmodel.predict({\"input_ids\": dev_encode[\"input_ids\"], 'attention_mask': dev_encode[\"attention_mask\"]})\n",
    "# predictionzz = (predictionzz > 0.5).astype(\"int32\")\n",
    "# predictionzz = np.ndarray.flatten(predictionzz)\n",
    "# print(\"Bert+2e5 on dev set\")\n",
    "# print(classification_report(dev_label, predictionzz, digits=4))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c568c11536c468db92cc2152f72bb087550d740e412a4a332b4e9cdb4eb1d017"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
