{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = \"train.data.txt\"\n",
    "dev_file = \"dev.data.txt\"\n",
    "test_file = \"test.data.txt\"\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('./%s.csv'%train_file,keep_default_na=False)\n",
    "dev_data = pd.read_csv('./%s.csv'%dev_file,keep_default_na=False)\n",
    "test_data = pd.read_csv('./%s.csv'%test_file,keep_default_na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import scipy\n",
    "import os\n",
    "import keras\n",
    "import string\n",
    "import tensorflow as tf\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "le = LabelEncoder()\n",
    "vectorizer = TfidfVectorizer()\n",
    "def tokenize_tweet(string_data:str):\n",
    "    stem = PorterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = nltk.RegexpTokenizer('\\w+')\n",
    "    data = string_data.replace('\\n', '')\n",
    "    data = data.lower()\n",
    "    data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n",
    "    data = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "    # data = re.sub('\\w*\\d\\w*', '', data)\n",
    "\n",
    "    data = tokenized.tokenize(data)\n",
    "    data = [i for i in data if i not in stopwords]\n",
    "    data = [stem.stem(word) for word in data]\n",
    "    data = [wordnet_lemmatizer.lemmatize(word) for word in data]\n",
    "    data = ' '.join(data)   \n",
    "    return data\n",
    "    \n",
    "def preprocess_token(df, dropNa=True):\n",
    "    data = df.copy()\n",
    "    data['main_tweet'] = data['main_tweet'].fillna('')\n",
    "    if dropNa:\n",
    "        data.replace('', np.nan, inplace=True)\n",
    "        data.dropna(subset=['main_tweet'], inplace=True)\n",
    "    data['main_tweet'] = data['main_tweet'].apply(lambda x: tokenize_tweet(x))\n",
    "    data['verified'] = data['verified'].astype(int)\n",
    "    return data\n",
    "\"\"\"\n",
    "    Preprocessing the training data\n",
    "\"\"\"\n",
    "\n",
    "train = preprocess_token(train_data)\n",
    "traintfidf = vectorizer.fit_transform(train['main_tweet'].tolist())\n",
    "traintfidf = scipy.sparse.csr_matrix(traintfidf).todense()\n",
    "d = pd.DataFrame(traintfidf)\n",
    "verified = pd.DataFrame({\"verified\":train['verified'].tolist()})\n",
    "followers = pd.DataFrame({\"followers\":train['followers'].tolist()})\n",
    "train_label = le.fit_transform(train['label'].tolist())\n",
    "train = pd.concat([d, verified], axis=1)\n",
    "train = tf.convert_to_tensor(train)\n",
    "\n",
    "\"\"\"\n",
    "    Preprocessing the dev data\n",
    "\"\"\"\n",
    "dev = preprocess_token(dev_data)\n",
    "devtfidf = vectorizer.transform(dev['main_tweet'].tolist())\n",
    "devtfidf = scipy.sparse.csr_matrix(devtfidf).todense()\n",
    "devpd = pd.DataFrame(devtfidf)\n",
    "dev_label=le.transform(dev['label'].tolist())\n",
    "dev_verified = pd.DataFrame({\"verified\":dev['verified'].tolist()})\n",
    "dev_followers = pd.DataFrame({\"followers\":dev['followers'].tolist()})\n",
    "dev = pd.concat([devpd, dev_verified], axis=1)\n",
    "dev = tf.convert_to_tensor(dev)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Preprocessing the test data\n",
    "\"\"\"\n",
    "test = preprocess_token(test_data, dropNa=False)\n",
    "testtfidf = vectorizer.transform(test['main_tweet'].tolist())\n",
    "testtfidf = scipy.sparse.csr_matrix(testtfidf).todense()\n",
    "testpd = pd.DataFrame(testtfidf)\n",
    "test_verified = pd.DataFrame({\"verified\":test['verified'].tolist()})\n",
    "test_followers = pd.DataFrame({\"followers\":test['followers'].tolist()})\n",
    "test = pd.concat([testpd, test_verified], axis=1)\n",
    "test = tf.convert_to_tensor(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devx = preprocess_token(dev_data,dropNa=False)\n",
    "devtfidfx = vectorizer.transform(devx['main_tweet'].tolist())\n",
    "devtfidfx = scipy.sparse.csr_matrix(devtfidfx).todense()\n",
    "devpdx = pd.DataFrame(devtfidfx)\n",
    "dev_labelx=le.transform(devx['label'].tolist())\n",
    "dev_verifiedx = pd.DataFrame({\"verified\":devx['verified'].tolist()})\n",
    "dev_followersx = pd.DataFrame({\"followers\":devx['followers'].tolist()})\n",
    "devx = pd.concat([devpdx, dev_verifiedx], axis=1)\n",
    "devx = tf.convert_to_tensor(devx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_tweet(\"Does the new coronavirus affect older people, or are younger people also susceptible? #Covid_19 #health https://t.co/Kg0Qb5oMs8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',input_shape=(train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train, train_label,epochs=20, validation_data=(dev, dev_label) ,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test)\n",
    "prediction = (prediction > 0.5).astype(\"int32\")\n",
    "prediction = np.ndarray.flatten(prediction)\n",
    "pd.DataFrame({\"Predicted\":  prediction}).to_csv('TFIDFNNVerified.csv', index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = model.predict(devx)\n",
    "# prediction = (prediction > 0.5).astype(\"int32\")\n",
    "# prediction = np.ndarray.flatten(prediction)\n",
    "# listo = []\n",
    "# for predict in prediction:\n",
    "#     if predict == 0:\n",
    "#         listo.append(\"nonrumor\")\n",
    "#     else:\n",
    "#         listo.append(\"rumor\")\n",
    "# newlisto = pd.DataFrame(listo)\n",
    "# newlisto\n",
    "# pd.DataFrame({\"Predicted\":  listo}).to_csv('TFIDFNNVerified.csv', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictionx = model.predict(dev)\n",
    "predictionx = (predictionx > 0.5).astype(\"int32\")\n",
    "predictionx = np.ndarray.flatten(predictionx)\n",
    "print(\"TFIDFNNVerified on dev set\")\n",
    "print(classification_report(dev_label, predictionx, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "\n",
    "# def update_data(df, filename):\n",
    "#     copy_test = df.copy()\n",
    "#     data_empty_test = copy_test[copy_test['main_tweet'].isna()]\n",
    "#     listid = data_empty_test['main_tweet_id'].tolist()\n",
    "#     print(len(listid))\n",
    "#     i = 0\n",
    "#     for id in listid:\n",
    "#         file = './project-data/tweet-objects/tweet-objects/%s.json'%id\n",
    "#         if os.path.exists(file):\n",
    "#             with open(file, 'r') as json_file:\n",
    "#                 data = json.load(json_file)\n",
    "#                 copy_test.loc[copy_test['main_tweet_id'] == id, 'verified'] = data['user']['verified']\n",
    "#                 copy_test.loc[copy_test['main_tweet_id'] == id, 'followers'] = data['user']['followers_count']\n",
    "#                 copy_test.loc[copy_test['main_tweet_id'] == id, 'main_tweet'] = data['text']\n",
    "#                 i+=1\n",
    "\n",
    "#     copy_test.to_csv(filename, index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t2 = pd.read_csv('train.data.txt.csv')\n",
    "# t3 = pd.read_csv('dev.data.txt.csv')\n",
    "# t4 = pd.read_csv('test.data.txt.csv')\n",
    "# update_data(t2, 'train.data.txt.csv')\n",
    "# update_data(t3, 'dev.data.txt.csv')\n",
    "# update_data(t4, 'test.data.txt.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b940357b9cf62d599d699ee25a438ac27bca5191110d61229d0fa3233193376"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
