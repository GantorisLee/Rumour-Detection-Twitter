{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = \"train.data.txt\"\n",
    "dev_file = \"dev.data.txt\"\n",
    "test_file = \"test.data.txt\"\n",
    "\n",
    "\n",
    "train_data = pd.read_csv('./%s.csv'%train_file,keep_default_na=False)\n",
    "dev_data = pd.read_csv('./%s.csv'%dev_file,keep_default_na=False)\n",
    "test_data = pd.read_csv('./%s.csv'%test_file,keep_default_na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import scipy\n",
    "import os\n",
    "import keras\n",
    "import string\n",
    "import tensorflow as tf\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "le = LabelEncoder()\n",
    "vectorizer = TfidfVectorizer()\n",
    "def tokenize_tweet(string_data:str):\n",
    "    stem = PorterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenized = nltk.RegexpTokenizer('\\w+')\n",
    "    data = string_data.replace('\\n', '')\n",
    "    data = data.lower()\n",
    "    data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n",
    "    data = re.sub('[%s]' % re.escape(string.punctuation), '', data)\n",
    "    data = re.sub('\\w*\\d\\w*', '', data)\n",
    "    #remove links\n",
    "    # print(data)\n",
    "    data = tokenized.tokenize(data)\n",
    "    data = [i for i in data if i not in stopwords]\n",
    "    data = [stem.stem(word) for word in data]\n",
    "    data = [wordnet_lemmatizer.lemmatize(word) for word in data]\n",
    "    data = ' '.join(data)   \n",
    "    return data\n",
    "def preprocess_token(df, dropNa=True):\n",
    "    data = df.copy()\n",
    "    data['main_tweet'] = data['main_tweet'].fillna('')\n",
    "    if dropNa:\n",
    "        data.replace('', np.nan, inplace=True)\n",
    "        data.dropna(subset=['main_tweet'], inplace=True)\n",
    "    data['main_tweet'] = data['main_tweet'].apply(lambda x: tokenize_tweet(x))\n",
    "    data['verified'] = data['verified'].astype(int)\n",
    "    return data\n",
    "\"\"\"\n",
    "    Preprocessing the training data\n",
    "\"\"\"\n",
    "\n",
    "train = preprocess_token(train_data)\n",
    "traintfidf = vectorizer.fit_transform(train['main_tweet'].tolist())\n",
    "traintfidf = scipy.sparse.csr_matrix(traintfidf).todense()\n",
    "d = pd.DataFrame(traintfidf)\n",
    "verified = pd.DataFrame({\"verified\":train['verified'].tolist()})\n",
    "followers = pd.DataFrame({\"followers\":train['followers'].tolist()})\n",
    "train_label = le.fit_transform(train['label'].tolist())\n",
    "train = pd.concat([d, verified], axis=1)\n",
    "train = tf.convert_to_tensor(train)\n",
    "\n",
    "\"\"\"\n",
    "    Preprocessing the dev data\n",
    "\"\"\"\n",
    "dev = preprocess_token(dev_data)\n",
    "devtfidf = vectorizer.transform(dev['main_tweet'].tolist())\n",
    "devtfidf = scipy.sparse.csr_matrix(devtfidf).todense()\n",
    "devpd = pd.DataFrame(devtfidf)\n",
    "dev_label=le.transform(dev['label'].tolist())\n",
    "dev_verified = pd.DataFrame({\"verified\":dev['verified'].tolist()})\n",
    "dev_followers = pd.DataFrame({\"followers\":dev['followers'].tolist()})\n",
    "dev = pd.concat([devpd, dev_verified], axis=1)\n",
    "dev = tf.convert_to_tensor(dev)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Preprocessing the test data\n",
    "\"\"\"\n",
    "test = preprocess_token(test_data, dropNa=False)\n",
    "testtfidf = vectorizer.transform(test['main_tweet'].tolist())\n",
    "testtfidf = scipy.sparse.csr_matrix(testtfidf).todense()\n",
    "testpd = pd.DataFrame(testtfidf)\n",
    "test_verified = pd.DataFrame({\"verified\":test['verified'].tolist()})\n",
    "test_followers = pd.DataFrame({\"followers\":test['followers'].tolist()})\n",
    "test = pd.concat([testpd, test_verified], axis=1)\n",
    "test = tf.convert_to_tensor(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_62 (Dense)            (None, 128)               598272    \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 608,641\n",
      "Trainable params: 608,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.3888 - accuracy: 0.8264 - val_loss: 0.2212 - val_accuracy: 0.9196\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.0575 - accuracy: 0.9860 - val_loss: 0.2722 - val_accuracy: 0.9215\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.2059 - val_accuracy: 0.9327\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 5.0611e-04 - accuracy: 1.0000 - val_loss: 0.2079 - val_accuracy: 0.9346\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 2.6113e-04 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9364\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.6211e-04 - accuracy: 1.0000 - val_loss: 0.2150 - val_accuracy: 0.9327\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.0970e-04 - accuracy: 1.0000 - val_loss: 0.2179 - val_accuracy: 0.9346\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 7.8770e-05 - accuracy: 1.0000 - val_loss: 0.2211 - val_accuracy: 0.9346\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 5.8902e-05 - accuracy: 1.0000 - val_loss: 0.2237 - val_accuracy: 0.9346\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 4.5518e-05 - accuracy: 1.0000 - val_loss: 0.2268 - val_accuracy: 0.9346\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 3.5955e-05 - accuracy: 1.0000 - val_loss: 0.2295 - val_accuracy: 0.9346\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 2.8853e-05 - accuracy: 1.0000 - val_loss: 0.2323 - val_accuracy: 0.9346\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 2.3475e-05 - accuracy: 1.0000 - val_loss: 0.2345 - val_accuracy: 0.9346\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.9501e-05 - accuracy: 1.0000 - val_loss: 0.2371 - val_accuracy: 0.9364\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.6137e-05 - accuracy: 1.0000 - val_loss: 0.2392 - val_accuracy: 0.9364\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.3533e-05 - accuracy: 1.0000 - val_loss: 0.2413 - val_accuracy: 0.9364\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.1436e-05 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9364\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 9.7659e-06 - accuracy: 1.0000 - val_loss: 0.2451 - val_accuracy: 0.9364\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 8.3773e-06 - accuracy: 1.0000 - val_loss: 0.2472 - val_accuracy: 0.9383\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 7.2016e-06 - accuracy: 1.0000 - val_loss: 0.2491 - val_accuracy: 0.9383\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation='relu',input_shape=(train.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train, train_label,epochs=20, validation_data=(dev, dev_label) ,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test)\n",
    "prediction = (prediction > 0.5).astype(\"int32\")\n",
    "prediction = np.ndarray.flatten(prediction)\n",
    "pd.DataFrame({\"Predicted\":  prediction}).to_csv('submission.csv', index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "\n",
    "# def update_data(df, filename):\n",
    "#     copy_test = df.copy()\n",
    "#     data_empty_test = copy_test[copy_test['main_tweet'].isna()]\n",
    "#     listid = data_empty_test['main_tweet_id'].tolist()\n",
    "#     print(len(listid))\n",
    "#     i = 0\n",
    "#     for id in listid:\n",
    "#         file = './project-data/tweet-objects/tweet-objects/%s.json'%id\n",
    "#         if os.path.exists(file):\n",
    "#             with open(file, 'r') as json_file:\n",
    "#                 data = json.load(json_file)\n",
    "#                 copy_test.loc[copy_test['main_tweet_id'] == id, 'verified'] = data['user']['verified']\n",
    "#                 copy_test.loc[copy_test['main_tweet_id'] == id, 'followers'] = data['user']['followers_count']\n",
    "#                 copy_test.loc[copy_test['main_tweet_id'] == id, 'main_tweet'] = data['text']\n",
    "#                 i+=1\n",
    "\n",
    "#     copy_test.to_csv(filename, index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t2 = pd.read_csv('train.data.txt.csv')\n",
    "# t3 = pd.read_csv('dev.data.txt.csv')\n",
    "# t4 = pd.read_csv('test.data.txt.csv')\n",
    "# update_data(t2, 'train.data.txt.csv')\n",
    "# update_data(t3, 'dev.data.txt.csv')\n",
    "# update_data(t4, 'test.data.txt.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c568c11536c468db92cc2152f72bb087550d740e412a4a332b4e9cdb4eb1d017"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
